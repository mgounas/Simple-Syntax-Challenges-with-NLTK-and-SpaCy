# Simple-Syntax-Tasks-with-NLTK-and-SpaCy
The Natural Language Toolkit, abbreviated as NLTK, is very useful for beginners to Natural Language Processing (NLP). It includes several features that help you prepare your data quickly and efficiently while programming in Python which is available to download here: https://www.python.org/downloads/.

# Tokenization
Tokenization is the process by which sentences are divided into their parts to be more easily analyzed. These parts are called *tokens* in NLP, but tokens often map to the words of sentences directly. Tokens are useful for identifying patterns in your data and are an important first step for many projects; rarely can we reach conclusions about a sentence, paragraph, or paper as a whole, if we cannot make conclusions about its parts. Tokens are our building blocks for text classfication, sentiment analysis, and more.

Tokenization is simplified with NLTK's **tokenize** module, which includes two sub-modules *word_tokenize()* and *sentence_tokenize()*.

## word_tokenize()


## sentence_tokenize()
